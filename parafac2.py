from scipy.io import loadmat
from data import irregular_tensor
import math
import torch
from tqdm import tqdm
import numpy as np

class parafac2:
    
    def __init__(self, _tensor, device, args):      
        # Intialization
        self.device = device
        self.tensor = _tensor
        factor_mat = loadmat(args.factor_path)
        _U = factor_mat['U'][0, :]
        self.S, self.V = factor_mat['S'], factor_mat['V']
        self.rank = self.S.shape[1]        
        
        self.U = np.zeros((_tensor.k, _tensor.i_max, self.rank))   # k x i_max x rank
        self.U_mask = torch.zeros((_tensor.k, _tensor.i_max, self.rank), device=device, dtype=torch.double)   # k x i_max x rank, 1 means the valid area
        self.centroids = torch.rand((_tensor.i_max, self.rank), device=device, dtype=torch.double)    # cluster centers
        self.centroids = torch.nn.Parameter(self.centroids)
        for _k in range(_tensor.k):
            self.U[_k, :_tensor.i[_k], :] = _U[_k]
            self.U_mask[_k, :_tensor.i[_k], :] = 1
                        
        # Upload to gpu        
        self.U = torch.tensor(self.U, device=device)
        self.S, self.V = torch.tensor(self.S, device=device), torch.tensor(self.V, device=device)        
        self.U = torch.nn.Parameter(self.U)
        self.S, self.V = torch.nn.Parameter(self.S), torch.nn.Parameter(self.V)        
        with torch.no_grad():
            print(f'fitness: {1 - math.sqrt(self.L2_loss(args.batch_size, self.U))/math.sqrt(self.tensor.sq_sum)}') 
            print(f'square loss: {self.L2_loss(args.batch_size, self.U)}')
        
        
                    
    def L2_loss(self, batch_size, input_U):        
        # V * sigma * H^T
        temp_tensor = self.V.repeat(self.tensor.k, 1, 1)  # k x F x rank
        temp_tensor = temp_tensor * self.S.unsqueeze(1)   # k x F x rank        
        temp_tensor_t = torch.transpose(temp_tensor, 1, 2)
        temp_tensor = torch.matmul(temp_tensor_t, temp_tensor)  # k x rank x rank
        
        # Computing the sum of square of tensors generated by parafac2
        temp_tensor = torch.matmul(input_U, temp_tensor)    # k x i_max x rank
        sq_sum = torch.sum(temp_tensor * input_U)
        
        # Correct non-zero terms
        for i in range(0, self.tensor.num_nnz, batch_size):
            if self.tensor.num_nnz - i < batch_size:
                curr_batch_size = self.tensor.num_nnz - i
            else:
                curr_batch_size = batch_size
                
            # Prepare matrices
            _row = self.tensor.rows[i:i+curr_batch_size]
            _col = self.tensor.cols[i:i+curr_batch_size]
            _height = self.tensor.heights[i:i+curr_batch_size]
            _vals = self.tensor.vals[i:i+curr_batch_size]
            
            _U = input_U[_height, _row, :]  # batch size x rank
            _S = self.S[_height, :]        # batch size x rank
            _V = self.V[_col, :]           # batch size x rank
            _approx = torch.sum(_U*_S*_V, dim=1)
            sq_sum = sq_sum - torch.sum(torch.square(_approx))
            sq_sum = sq_sum + torch.sum(torch.square(_approx - _vals))
            
        return sq_sum

    
    def clustering(self):
        # Clustering
        with torch.no_grad():
            dist = torch.zeros((self.tensor.i_max, self.tensor.k, self.tensor.i_max), device=self.device)    
            for i in range(self.tensor.i_max):
                curr_dist = self.U - self.centroids[i,:].unsqueeze(0).unsqueeze(0) # k x i_max x rank
                curr_dist = torch.sum(torch.square(curr_dist), dim=-1) # k x i_max
                dist[i,:,:] = curr_dist
            #dist = self.Q.repeat(self.tensor.i_max, 1, 1, 1) - self.centroids.unsqueeze(1).unsqueeze(1)  # i_max x k x i_max x rank
            #dist = torch.sum(torch.square(dist), dim=-1) # i_max x k x i_max
            cluster_label = torch.argmin(dist, dim=0)  # k x i_max
        return cluster_label
        
        
    def quantization(self, args):
        optimizer = torch.optim.Adam([self.U, self.S, self.V, self.centroids], lr=args.lr)
        
        for _epoch in tqdm(range(args.epoch)):
            optimizer.zero_grad()
            # Clustering     
            U_clustered = self.centroids[self.clustering()]    # k x i_max x rank
            U_clustered = U_clustered * self.U_mask
            sg_part = (self.U - U_clustered).detach()
            U_tricked = self.U - sg_part # refer to paper   # k x i_max x rank
            _loss = self.L2_loss(args.batch_size, U_tricked) + torch.sum(torch.square(U_clustered - self.U.detach()))
            _loss.backward()
            optimizer.step()
            
            if (_epoch + 1) % 10 == 0:
                with torch.no_grad():
                    U_clustered = self.centroids[self.clustering()]    # k x i_max x rank
                    U_clustered = U_clustered * self.U_mask
                    _loss = self.L2_loss(args.batch_size, U_clustered)
                    _fitness = 1 - math.sqrt(_loss)/math.sqrt(self.tensor.sq_sum)
                    print(f'epoch: {_epoch}, l2 loss: {_loss}, fitness: {_fitness}')
                    with open(args.output_path, 'a') as f:
                        f.write(f'epoch: {_epoch}, l2 loss: {_loss}, fitness: {_fitness}\n')

#device = torch.device("cuda:0")
#_tensor = irregular_tensor('../input/23-Irregular-Tensor/cms_sample.npy', device)
#_parafac2 = parafac2(_tensor, 'parafac2/cms_factor.mat', device)     